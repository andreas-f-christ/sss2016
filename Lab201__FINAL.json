{"paragraphs":[{"text":"%md\n\n# Introduction to Machine Learning with Apache Spark\nLab based on the *Hands-on* portion of the **[Future of Data](http://www.meetup.com/pro/futureofdata/)** Apache Spark meetup by @RobHryniewicz.\n","dateUpdated":"Sep 29, 2016 8:45:41 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333232_1919143855","id":"20160928-225533_723198853","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Introduction to Machine Learning with Apache Spark</h1>\n<p>Lab based on the <em>Hands-on</em> portion of the <strong><a href=\"http://www.meetup.com/pro/futureofdata/\">Future of Data</a></strong> Apache Spark meetup by @RobHryniewicz.</p>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:44","dateFinished":"Sep 29, 2016 8:45:44 AM","dateStarted":"Sep 29, 2016 8:45:41 AM","focus":true},{"title":"Introduction","text":"%md\n\nIn this lab you will run a few examples covering both *unsupervised* learning, such as K-Means clustering, as well as *supervised* learning, such as Decision Trees and Random Forests. The purpose of this lab is to get you started exploring machine learning algorithms without going into mathematical details of what goes on behind the scenes.\n#\nWe will cover several examples that are part of Apache Spark package using both the original Spark MLlib API as well as the newer Spark ML API.\n#\nOnce you're done, you should have a better feel for the powerful Machine Learning libraries that are part of Apache Spark.\n#\nFor a complete documentation checkout the official Apache Spark [Machine Learning Library (MLlib) Guide](http://spark.apache.org/docs/latest/mllib-guide.html).\n#","dateUpdated":"Sep 29, 2016 8:45:47 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333249_1998786877","id":"20160928-225533_1634726083","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In this lab you will run a few examples covering both <em>unsupervised</em> learning, such as K-Means clustering, as well as <em>supervised</em> learning, such as Decision Trees and Random Forests. The purpose of this lab is to get you started exploring machine learning algorithms without going into mathematical details of what goes on behind the scenes.</p>\n<h1></h1>\n<p>We will cover several examples that are part of Apache Spark package using both the original Spark MLlib API as well as the newer Spark ML API.</p>\n<h1></h1>\n<p>Once you're done, you should have a better feel for the powerful Machine Learning libraries that are part of Apache Spark.</p>\n<h1></h1>\n<p>For a complete documentation checkout the official Apache Spark <a href=\"http://spark.apache.org/docs/latest/mllib-guide.html\">Machine Learning Library (MLlib) Guide</a>.</p>\n<h1></h1>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:45","dateFinished":"Sep 29, 2016 8:45:48 AM","dateStarted":"Sep 29, 2016 8:45:48 AM","focus":true},{"title":"Programming Language: Scala","text":"%md\n\nThroughout this lab we will use basic Scala syntax. If you would like to learn more about Scala, here's an excellent [Scala Tutorial](http://www.dhgarrette.com/nlpclass/scala/basics.html).","dateUpdated":"Sep 29, 2016 8:45:58 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333250_1999941124","id":"20160928-225533_2082980910","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Throughout this lab we will use basic Scala syntax. If you would like to learn more about Scala, here's an excellent <a href=\"http://www.dhgarrette.com/nlpclass/scala/basics.html\">Scala Tutorial</a>.</p>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:46","dateFinished":"Sep 29, 2016 8:45:58 AM","dateStarted":"Sep 29, 2016 8:45:58 AM","focus":true},{"title":"Lab Pre-Check","text":"%md\n\nBefore we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.\n#\n**Note**: The first time you run `sc.version` in the paragraph below, several services will initialize in the background. This may take **1~2 min** so please **be patient**. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.","dateUpdated":"Sep 29, 2016 8:46:01 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333252_1997632630","id":"20160928-225533_381740976","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Before we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.</p>\n<h1></h1>\n<p><strong>Note</strong>: The first time you run <code>sc.version</code> in the paragraph below, several services will initialize in the background. This may take <strong>1~2 min</strong> so please <strong>be patient</strong>. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.</p>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:47","dateFinished":"Sep 29, 2016 8:46:02 AM","dateStarted":"Sep 29, 2016 8:46:02 AM","focus":true},{"text":"sc.version","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333253_1997247882","id":"20160928-225533_1386930043","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:48"},{"text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","dateUpdated":"Sep 29, 2016 8:49:49 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333255_1998017379","id":"20160928-225533_1345728163","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:49","dateFinished":"Sep 29, 2016 8:49:49 AM","dateStarted":"Sep 29, 2016 8:49:49 AM","focus":true},{"title":"Unsupervised Learning: K-Means Clustering","text":"%md\n\n#### Unsupervised Learning\n\n\"Unsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning and reinforcement learning.\" - [wikipedia](https://en.wikipedia.org/wiki/Unsupervised_learning)\n#\n#### K-Means Clustering\n\nK-Means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. (See [Spark docs](http://spark.apache.org/docs/latest/ml-clustering.html) for more info.)\n#\nWe will use Spark ML API to generate a K-Means model using the Spark ML KMeans class. \n#\nKMeans is implemented as an Estimator and generates a KMeansModel as the base model.\n#\nNote that the data points for the training are hardcoded in the example below. Before you run the K-Means sample code, try to guess what the two cluster centers should be based on the training data.\n#","dateUpdated":"Sep 29, 2016 8:46:06 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333256_1996093635","id":"20160928-225533_355465337","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Unsupervised Learning</h4>\n<p>&ldquo;Unsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning and reinforcement learning.&rdquo; - <a href=\"https://en.wikipedia.org/wiki/Unsupervised_learning\">wikipedia</a></p>\n<h1></h1>\n<h4>K-Means Clustering</h4>\n<p>K-Means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. (See <a href=\"http://spark.apache.org/docs/latest/ml-clustering.html\">Spark docs</a> for more info.)</p>\n<h1></h1>\n<p>We will use Spark ML API to generate a K-Means model using the Spark ML KMeans class.</p>\n<h1></h1>\n<p>KMeans is implemented as an Estimator and generates a KMeansModel as the base model.</p>\n<h1></h1>\n<p>Note that the data points for the training are hardcoded in the example below. Before you run the K-Means sample code, try to guess what the two cluster centers should be based on the training data.</p>\n<h1></h1>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:50","dateFinished":"Sep 29, 2016 8:46:07 AM","dateStarted":"Sep 29, 2016 8:46:07 AM","focus":true},{"text":"import org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.sql.{DataFrame, SQLContext} \n\nval sqlContext = new SQLContext(sc)\n\n// Crates a DataFrame\nval dataSmall: DataFrame = sqlContext.createDataFrame(Seq(\n  (1, Vectors.dense(0.0, 0.0, 0.0)),\n  (2, Vectors.dense(0.1, 0.1, 0.1)),\n  (3, Vectors.dense(0.2, 0.2, 0.2)),\n  (4, Vectors.dense(3.0, 3.0, 3.0)),\n  (5, Vectors.dense(3.1, 3.1, 3.1)),\n  (6, Vectors.dense(3.2, 3.2, 3.2))\n)).toDF(\"id\", \"features\")\n\ndataSmall.printSchema\n\n// Trains a k-means model\nval kmeans = new KMeans()\n  .setK(2)                              // set number of clusters\n  .setFeaturesCol(\"features\")\n  .setPredictionCol(\"prediction\")\nval model = kmeans.fit(dataSmall)\n\n// Shows the result\nprintln(\"Final Centers: \")\nmodel.clusterCenters.foreach(println)\n","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333258_1996863133","id":"20160928-225533_6413005","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:51"},{"text":"%md\n\nDid you guess the cluster centers correctly?\n#\nAlthough this is a very simple exmaple, it should provide you with an intuitive feel for K-Means clustering.\n#\nBelow we've provided you with a visualization of training data points and computed cluster centers.\n#","dateUpdated":"Sep 29, 2016 8:46:16 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333259_1996478384","id":"20160928-225533_409192685","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Did you guess the cluster centers correctly?</p>\n<h1></h1>\n<p>Although this is a very simple exmaple, it should provide you with an intuitive feel for K-Means clustering.</p>\n<h1></h1>\n<p>Below we've provided you with a visualization of training data points and computed cluster centers.</p>\n<h1></h1>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:52","dateFinished":"Sep 29, 2016 8:46:17 AM","dateStarted":"Sep 29, 2016 8:46:16 AM","focus":true},{"title":"Visualized Result of K-Means Clustering","text":"%md\n\nThe input data is marked with a blue **+** and the two K-Means cluser centers are marked with a red **x**.\n#\n![scatter-plot](https://raw.githubusercontent.com/roberthryniewicz/images/master/lab201-plt-3d-scatter.png)","dateUpdated":"Sep 29, 2016 8:46:21 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333261_1994169890","id":"20160928-225533_1058819101","result":{"code":"SUCCESS","type":"HTML","msg":"<p>The input data is marked with a blue <strong>+</strong> and the two K-Means cluser centers are marked with a red <strong>x</strong>.</p>\n<h1></h1>\n<p><img src=\"https://raw.githubusercontent.com/roberthryniewicz/images/master/lab201-plt-3d-scatter.png\" alt=\"scatter-plot\" /></p>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:53","dateFinished":"Sep 29, 2016 8:46:21 AM","dateStarted":"Sep 29, 2016 8:46:21 AM","focus":true},{"title":"Supervised Learning: Decision Trees and Random Forests","text":"%md\n\n### Supervised Learning\n\n\"Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way.\" - [wikipedia](https://en.wikipedia.org/wiki/Supervised_learning)","dateUpdated":"Sep 29, 2016 8:46:27 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333262_1995324137","id":"20160928-225533_1523131465","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Supervised Learning</h3>\n<p>&ldquo;Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way.&rdquo; - <a href=\"https://en.wikipedia.org/wiki/Supervised_learning\">wikipedia</a></p>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:54","dateFinished":"Sep 29, 2016 8:46:28 AM","dateStarted":"Sep 29, 2016 8:46:27 AM","focus":true},{"title":"Training Dataset","text":"%md\n\nFor Decision Tree and Random Forest examples we will use a diabetes dataset that has been cleansed, scaled, and sanitized to remove any personally identifying information. The diabetes dataset contains a distribution for 70 sets of data recorded on diabetes patients (several weeks' to months' worth of glucose, insulin, and lifestyle data per patient + a description of the problem domain).\n#\nKeep in mind that we are not particularly concerned what specific features represent, rather we will train our Decision Trees and Random Forest models to learn how the underlying features \"predict\" either negative or positive result based on the labeled training data set.\n#\n","dateUpdated":"Sep 29, 2016 8:46:33 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333264_2005327608","id":"20160928-225533_1265460015","result":{"code":"SUCCESS","type":"HTML","msg":"<p>For Decision Tree and Random Forest examples we will use a diabetes dataset that has been cleansed, scaled, and sanitized to remove any personally identifying information. The diabetes dataset contains a distribution for 70 sets of data recorded on diabetes patients (several weeks' to months' worth of glucose, insulin, and lifestyle data per patient + a description of the problem domain).</p>\n<h1></h1>\n<p>Keep in mind that we are not particularly concerned what specific features represent, rather we will train our Decision Trees and Random Forest models to learn how the underlying features &ldquo;predict&rdquo; either negative or positive result based on the labeled training data set.</p>\n<h1></h1>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:55","dateFinished":"Sep 29, 2016 8:46:33 AM","dateStarted":"Sep 29, 2016 8:46:33 AM","focus":true},{"title":"Download Dataset","text":"%sh\n\nwget --no-check-certificate http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/breast-cancer_scale -O /tmp/breast-cancer_scaled_data.txt","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333266_2006097106","id":"20160928-225533_2055915864","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"title":"Copy Dataset to HDFS and preview","text":"%sh\nhadoop fs -put /tmp/breast-cancer_scaled_data.txt /tmp\nhadoop fs -ls /tmp\nhadoop fs -tail /tmp/breast-cancer_scaled_data.txt","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333267_2005712357","id":"20160928-225533_766046018","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"title":"Count Number of Lines in the Dataset","text":"%pyspark\nmyLines = sc.textFile('hdfs://sandbox.hortonworks.com/tmp/breast-cancer_scaled_data.txt')\nmyLinesFiltered = myLines.filter( lambda x: len(x) > 0 )\ncount = myLinesFiltered.count()\nprint count","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/sh","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333269_2003403864","id":"20160928-225533_770960761","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"title":"Load Data into SPARK","text":"import org.apache.spark.mllib.util.MLUtils\n\n// Load and parse the data file (as DataFrame)\nval data = MLUtils.loadLibSVMFile(sc, \"hdfs://sandbox.hortonworks.com/tmp/breast-cancer_scaled_data.txt\")\nval dataDF = data.toDF()\nval dataRDD = dataDF.rdd.map(s => Vectors.dense(s.getDouble(1),s.getDouble(2))).cache()\n\ndataDF.printSchema\ndataDF.groupBy($\"label\").count().orderBy($\"count\".desc).registerTempTable(\"dataLabelCount\")\n","dateUpdated":"Sep 28, 2016 11:02:57 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333270_2004558111","id":"20160928-225533_1768667776","dateCreated":"Sep 28, 2016 10:55:33 PM","dateStarted":"Sep 28, 2016 11:02:57 PM","dateFinished":"Sep 28, 2016 11:03:35 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:59","errorMessage":""},{"title":"Show distribution of labels","text":"%sql \nSELECT * FROM dataLabelCount","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"title":true,"graph":{"mode":"pieChart","height":300,"optionOpen":false,"keys":[{"name":"label","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"label","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333272_2002249617","id":"20160928-225533_530669458","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"title":"Perform k-means","text":"\nimport org.apache.spark.ml.clustering.KMeans\n\n// Trains a k-means model\nval kmeans = new KMeans()\n  .setK(2)                              // set number of clusters\n  .setFeaturesCol(\"features\")\n  .setPredictionCol(\"prediction\")\nval kMeansModel = kmeans.fit(dataDF)\n\n// Evaluate clustering by computing Within Set Sum of Squared Errors.\nval WSSSE = kMeansModel.computeCost(dataDF)\nprintln(s\"Within Set Sum of Squared Errors = $WSSSE\")\n\n// Show cluster centers\nprintln(\"Cluster Centers\")\nkMeansModel.clusterCenters.foreach(println)\n\n// Use model to predict cluster\nval dataPredicted = kMeansModel.transform(dataDF)\n\n//dataPredicted.show(false)\ndataPredicted.registerTempTable(\"dataPredictedTable\")\n","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333275_2002634366","id":"20160928-225533_461994615","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"title":"Show k-means clustering results","text":"%sql\nSELECT label, prediction FROM dataPredictedTable ORDER BY label","dateUpdated":"Sep 29, 2016 8:50:55 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"label","index":0,"aggr":"sum"}],"values":[{"name":"prediction","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"label","index":0,"aggr":"sum"},"yAxis":{"name":"prediction","index":1,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":9},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333276_2000710622","id":"20160928-225533_1590110400","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"title":"Show k-means clustering results","text":"import org.apache.spark.sql.Row\nimport org.apache.spark.mllib.linalg.SparseVector\n\nval dim1 = 2\nval dim2 = 5\n\nprintln(\"%table x\\ty\\tcluster\")\ndataPredicted.collect().foreach { r =>\n println(Seq(r.getAs[SparseVector](1).toDense(dim1), r.getAs[SparseVector](1).toDense(dim2), r.getInt(2) ).mkString(\"\\t\"))\n}\n","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"title":true,"graph":{"mode":"scatterChart","height":300,"optionOpen":false,"keys":[{"name":"x","index":0,"aggr":"sum"}],"values":[{"name":"y","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"x","index":0,"aggr":"sum"},"yAxis":{"name":"y","index":1,"aggr":"sum"},"group":{"name":"cluster","index":2,"aggr":"sum"}}},"editorMode":"ace/mode/scala","colWidth":9},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333430_1647511131","id":"20160928-225533_1698989532","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"title":"Decision Trees","text":"%md\n\nDecision trees and their ensembles are popular methods for the machine learning tasks of classification and regression. Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks.\n#\nThe spark.ml implementation supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions or even billions of instances. ([See docs](http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-trees) for more info.)\n#\nMake sure to checkout **[this](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)** great introduction to Visual Machine Learning to get an intuitive feel for the *ideas* behind Decision Tree classification.","dateUpdated":"Sep 29, 2016 8:46:46 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333640_1749854339","id":"20160928-225533_1974493812","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Decision trees and their ensembles are popular methods for the machine learning tasks of classification and regression. Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks.</p>\n<h1></h1>\n<p>The spark.ml implementation supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions or even billions of instances. (<a href=\"http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-trees\">See docs</a> for more info.)</p>\n<h1></h1>\n<p>Make sure to checkout <strong><a href=\"http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\">this</a></strong> great introduction to Visual Machine Learning to get an intuitive feel for the <em>ideas</em> behind Decision Tree classification.</p>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:64","dateFinished":"Sep 29, 2016 8:46:46 AM","dateStarted":"Sep 29, 2016 8:46:46 AM","focus":true},{"text":"%md\n\nLet's train a Decision Tree model.\n\nWe will hold 30% of the data as a test dataset and will set a maximum tree depth to 5.\n\n","dateUpdated":"Sep 29, 2016 8:51:02 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333642_1758703563","id":"20160928-225533_252701193","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Let's train a Decision Tree model.</p>\n<p>We will hold 30% of the data as a test dataset and will set a maximum tree depth to 5.</p>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:65","dateFinished":"Sep 29, 2016 8:51:00 AM","dateStarted":"Sep 29, 2016 8:51:00 AM","focus":true},{"title":"Decision Trees with Spark ML","text":"import org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load and parse the data file (as DataFrame)\nval data = MLUtils.loadLibSVMFile(sc, \"hdfs://sandbox.hortonworks.com/tmp/breast-cancer_scaled_data.txt\")\n\n// re-map labels from {2, 4} to {0, 1} space. (Otherwise an error will occur.)\nval data_remapped = data.map(d => new LabeledPoint(if (d.label == 2) 0 else 1, (d.features).toDense))\n\n// Split the data into training and test sets (30% held out for testing)\nval splits = data_remapped.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n\n// Train a DecisionTree model.\n//  Empty categoricalFeaturesInfo indicates all features are continuous.\nval numClasses = 2\nval categoricalFeaturesInfo = Map[Int, Int]()\nval impurity = \"gini\"\nval maxDepth = 5\nval maxBins = 32\n\nval model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n  impurity, maxDepth, maxBins)\n\n// Evaluate model on test instances and compute test error\nval labelAndPreds = testData.map { point =>\n  val prediction = model.predict(point.features)\n  (point.label, prediction)\n}\nval testErr = labelAndPreds.filter(r => r._1 != r._2).count().toDouble / testData.count()\nprintln(\"Test Error = \" + testErr)\nprintln(\"Learned classification tree model:\\n\" + model.toDebugString)","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333651_1759473061","id":"20160928-225533_1573780632","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"text":"%md\n\nWhat's the accuracy when using Spark ML API? What happens if you re-run the algo? Is the accuracy still the same? \n#\nCan you guess why you get different results?\n#","dateUpdated":"Sep 29, 2016 8:46:55 AM","config":{"enabled":true,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333653_1757164568","id":"20160928-225533_2122485572","result":{"code":"SUCCESS","type":"HTML","msg":"<p>What's the accuracy when using Spark ML API? What happens if you re-run the algo? Is the accuracy still the same?</p>\n<h1></h1>\n<p>Can you guess why you get different results?</p>\n<h1></h1>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:67","dateFinished":"Sep 29, 2016 8:46:55 AM","dateStarted":"Sep 29, 2016 8:46:55 AM","focus":true},{"title":"Random Forests","text":"%md\n\nNow let's see if we can achieve a better performance with an ensemble of trees known as random forests. \n#\nRandom forests combine many decision trees in order to reduce the risk of overfitting. The spark.ml implementation supports random forests for binary and multiclass classification and for regression, using both continuous and categorical features. ([See docs](http://spark.apache.org/docs/latest/ml-classification-regression.html#random-forests) for more info.)\n#\nIn the example below we will combine five (5) trees to create a forest of trees.\n#\n","dateUpdated":"Sep 29, 2016 8:47:00 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333656_1756010321","id":"20160928-225533_764148287","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's see if we can achieve a better performance with an ensemble of trees known as random forests.</p>\n<h1></h1>\n<p>Random forests combine many decision trees in order to reduce the risk of overfitting. The spark.ml implementation supports random forests for binary and multiclass classification and for regression, using both continuous and categorical features. (<a href=\"http://spark.apache.org/docs/latest/ml-classification-regression.html#random-forests\">See docs</a> for more info.)</p>\n<h1></h1>\n<p>In the example below we will combine five (5) trees to create a forest of trees.</p>\n<h1></h1>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:68","dateFinished":"Sep 29, 2016 8:47:00 AM","dateStarted":"Sep 29, 2016 8:47:00 AM","focus":true},{"title":"Random Forest with Spark ML","text":"import org.apache.spark.sql.SQLContext                                                                                                  \n\nimport org.apache.spark.ml.Pipeline                                                                                                     \nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}                                     \nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator                                                                 \nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}                                                        \n\nval sqlContext = new SQLContext(sc)                                                                                                 \n                                                                                                                                        \n// Load and parse the LIBSVM data file, converting it to a DataFrame.\nval data = MLUtils.loadLibSVMFile(sc, \"hdfs://sandbox.hortonworks.com/tmp/breast-cancer_scaled_data.txt\").toDF()\n\n                                                                                                                                        \n// Index labels, adding metadata to the label column.                                                                               \n// Fit on whole dataset to include all labels in index.                                                                             \nval labelIndexer = new StringIndexer()                                                                                              \n      .setInputCol(\"label\")                                                                                                             \n      .setOutputCol(\"indexedLabel\")                                                                                                     \n      .fit(data)                                                                                                                        \n\n// Automatically identify categorical features, and index them.                                                                     \nval featureIndexer = new VectorIndexer()                                                                                            \n      .setInputCol(\"features\")                                                                                                          \n      .setOutputCol(\"indexedFeatures\")                                                                                                  \n      .setMaxCategories(2)                                                                                                              \n      .fit(data)                                                                                                                        \n                                                                                                                                        \n// Split the data into training and test sets (30% held out for testing)                                                            \nval Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))                                                               \n                                                                                                                                        \n// Train a RandomForest model.                                                                                                      \nval rf = new RandomForestClassifier()                                                                                               \n      .setLabelCol(\"indexedLabel\")                                                                                                      \n      .setFeaturesCol(\"indexedFeatures\")                                                                                                \n      .setNumTrees(5)                                                                                                                  \n                                                                                                                                        \n// Convert indexed labels back to original labels.                                                                                  \nval labelConverter = new IndexToString()                                                                                            \n      .setInputCol(\"prediction\")                                                                                                        \n      .setOutputCol(\"predictedLabel\")                                                                                                   \n      .setLabels(labelIndexer.labels)                                                                                                   \n                                                                                                                                        \n// Chain indexers and forest in a Pipeline                                                                                          \nval pipeline = new Pipeline()                                                                                                       \n      .setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))                                                               \n                                                                                                                                        \n// Train model.  This also runs the indexers.                                                                                       \nval model = pipeline.fit(trainingData)                                                                                              \n                                                                                                                                        \n// Make predictions.                                                                                                                \nval predictions = model.transform(testData)                                                                                         \n                                                                                                                                        \n// Select example rows to display.                                                                                                  \npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)                                                                   \n                                                                                                                                        \n// Select (prediction, true label) and compute test error                                                                           \nval evaluator = new MulticlassClassificationEvaluator()                                                                             \n      .setLabelCol(\"indexedLabel\")                                                                                                      \n      .setPredictionCol(\"prediction\")                                                                                                   \n      .setMetricName(\"precision\")                                                                                                       \n    \nval accuracy = evaluator.evaluate(predictions)                                                                                      \nprintln(\"Test Error = \" + (1.0 - accuracy))                                                                                         \n                                                                                                                                        \nval rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]                                                         \nprintln(\"Learned classification forest model:\\n\" + rfModel.toDebugString)  ","dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333657_1756779819","id":"20160928-225533_1088050549","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"text":"%md\n#\nHas the accuracy improved after training the dataset using the Random Forest model? Does the accuracy improve if you increase the number of trees?\n#\nWhat did you find interesting in the output of a Random Forest classifier?\n#","dateUpdated":"Sep 29, 2016 8:47:10 AM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333659_1756395070","id":"20160928-225533_1042322838","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>Has the accuracy improved after training the dataset using the Random Forest model? Does the accuracy improve if you increase the number of trees?</p>\n<h1></h1>\n<p>What did you find interesting in the output of a Random Forest classifier?</p>\n<h1></h1>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:70","dateFinished":"Sep 29, 2016 8:47:10 AM","dateStarted":"Sep 29, 2016 8:47:10 AM","focus":true},{"title":"Feature Selection","text":"import org.apache.spark.mllib.feature.ChiSqSelector\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nval discretizedData = data.map { lp => \n   LabeledPoint(lp.label, Vectors.dense(lp.features.toArray.map { x => (x * 10).floor })) }\nval selector = new ChiSqSelector(2)\nval transformer = selector.fit( discretizedData )\n\n// Filter the top features from each feature vector\nval filteredData = discretizedData.map { lp =>\n    LabeledPoint(lp.label, transformer.transform(lp.features))\n}","dateUpdated":"Sep 29, 2016 8:48:51 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333661_1754086577","id":"20160928-225533_1295829687","dateCreated":"Sep 28, 2016 10:55:33 PM","dateStarted":"Sep 28, 2016 11:03:42 PM","dateFinished":"Sep 28, 2016 11:03:54 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:71","errorMessage":""},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475138856649_-1709541574","id":"20160929-084736_1226742357","dateCreated":"Sep 29, 2016 8:47:36 AM","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:777","dateUpdated":"Sep 29, 2016 8:49:07 AM","dateFinished":"Sep 29, 2016 8:49:01 AM","dateStarted":"Sep 29, 2016 8:49:01 AM","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Can you create a 2d scatter plot of the reduced data?</p>\n"},"text":"%md\n\nCan you create a 2d scatter plot of the reduced data?"},{"title":"The End","text":"%md\n\nThis concludes our lab. Hopefully you've got a taste of how easy it is to run clustering and classification models with Apache Spark!","dateUpdated":"Sep 29, 2016 8:49:12 AM","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333662_1755240823","id":"20160928-225533_1377885182","result":{"code":"SUCCESS","type":"HTML","msg":"<p>This concludes our lab. Hopefully you've got a taste of how easy it is to run clustering and classification models with Apache Spark!</p>\n"},"dateCreated":"Sep 28, 2016 10:55:33 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:72","dateFinished":"Sep 29, 2016 8:47:59 AM","dateStarted":"Sep 29, 2016 8:47:59 AM","focus":true},{"dateUpdated":"Sep 28, 2016 10:55:33 PM","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1475103333664_1740620365","id":"20160928-225533_1800088273","dateCreated":"Sep 28, 2016 10:55:33 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:73"}],"name":"Lab201","id":"2BZP3452Q","angularObjects":{"2BYF9D6QV":[],"2BZHJQE5V":[],"2BZNP3DXF":[],"2BX2Z3ZH3":[],"2BWQHEQW6":[],"2BXYWM812":[],"2BX2Z6WH9":[],"2BYQ1P15U":[],"2BWX1FFBR":[],"2BXM539DX":[],"2BZANFD2R":[],"2BYVWA6JH":[],"2BXPHCNT7":[]},"config":{"looknfeel":"default"},"info":{}}